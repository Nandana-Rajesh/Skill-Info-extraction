{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing JSON files\n",
    "data_directory =r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\ner-label\\ner-label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed alabelled.json\n",
      "Processed annotations (1).json\n",
      "Processed annotations (2).json\n",
      "Processed annotations (3).json\n",
      "Processed annotations.json\n",
      "Processed melabelled.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Define the function to process all JSON files in a folder\n",
    "def process_json_files(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            # Construct full file path\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read JSON data from file\n",
    "            with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            # Apply clean_text function to the text field in JSON data\n",
    "            if 'text' in data:\n",
    "                data['text'] = clean_text(data['text'])\n",
    "            \n",
    "            # Construct output file path\n",
    "            output_filepath = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Save cleaned data to new JSON file\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\ner-label\\ner-label\"\n",
    "output_folder = r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\ner-label\\OUTPUT\"\n",
    "process_json_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.qualifications  essential  bachelor degree des...\" with entities \"[(122, 128, 'LOCATION'), (131, 139, 'LOCATION'), (...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" health care services, admin, team building, nurs...\" with entities \"[(2, 29, 'JOB_TITLE'), (46, 53, 'JOB_TITLE'), (55,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.will effectively collaborate with the recruite...\" with entities \"[(724, 745, 'SKILL'), (778, 790, 'SKILL'), (802, 8...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" kindly apply after reading this post thoroughly....\" with entities \"[(97, 106, 'LOCATION'), (295, 303, 'EXPERIENCE'), ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" position title assistant product manager / produ...\" with entities \"[(2065, 2079, 'SALARY'), (2082, 2094, 'SALARY'), (...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" medical coding fresher required. day shift job. ...\" with entities \"[(2, 16, 'JOB TITLE'), (108, 135, 'SALARY'), (253,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" job profile  day shift & 5 days a week respondin...\" with entities \"[(238, 259, 'EXPERIENCE'), (264, 277, 'EXPERIENCE'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" ra api salary  not disclosed by recruiter indust...\" with entities \"[(54, 60, 'INDUSTRY'), (63, 70, 'INDUSTRY'), (73, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" location  waverock sez, hyderabad department  te...\" with entities \"[(26, 35, 'LOCATION'), (120, 182, 'SKILL'), (214, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" greetings from pace recruitment    urgent openin...\" with entities \"[(56, 87, 'JOB_TITLE'), (115, 124, 'LOCATION'), (2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" analysis and solution definition quickly underst...\" with entities \"[(995, 1008, 'SKILL'), (1192, 1207, 'SKILL'), (137...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" our client is the largest beauty services market...\" with entities \"[(946, 948, 'SKILL'), (950, 957, 'SKILL'), (959, 9...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" medical coding is the process of conversion of t...\" with entities \"[(214, 242, 'SALARY'), (302, 309, 'INDUSTRY'), (31...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" ideal candidate is someone who is very smart, a ...\" with entities \"[(209, 216, 'EXPERIENCE'), (508, 512, 'SKILL'), (5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" name of the client reputed engineering college ,...\" with entities \"[(105, 118, 'EXPERIENCE'), (177, 187, 'LOCATION'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.our company has got good cnc machines.. 2. we ...\" with entities \"[(68, 94, 'JOB TITLE'), (100, 130, 'SKILL '), (288...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" to carryout day to day microbiological analysis ...\" with entities \"[(25, 49, 'JOB TITLE'), (511, 543, 'QUALIFICATION'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 20297.14560275765}\n",
      "Iteration 2, Losses: {'ner': 6961.395080020997}\n",
      "Iteration 3, Losses: {'ner': 6872.456232683886}\n",
      "Iteration 4, Losses: {'ner': 5990.108385942216}\n",
      "Iteration 5, Losses: {'ner': 6166.0436911286615}\n",
      "Iteration 6, Losses: {'ner': 6462.549347239395}\n",
      "Iteration 7, Losses: {'ner': 5723.495128085717}\n",
      "Iteration 8, Losses: {'ner': 5620.439736403055}\n",
      "Iteration 9, Losses: {'ner': 5382.95891438455}\n",
      "Iteration 10, Losses: {'ner': 5599.2879178555995}\n",
      "Iteration 11, Losses: {'ner': 6068.9104646088335}\n",
      "Iteration 12, Losses: {'ner': 5544.117865352066}\n",
      "Iteration 13, Losses: {'ner': 5359.392727988822}\n",
      "Iteration 14, Losses: {'ner': 5329.947967158894}\n",
      "Iteration 15, Losses: {'ner': 5739.991308677103}\n",
      "Iteration 16, Losses: {'ner': 5706.215741264032}\n",
      "Iteration 17, Losses: {'ner': 5824.125607779648}\n",
      "Iteration 18, Losses: {'ner': 6107.399210715988}\n",
      "Iteration 19, Losses: {'ner': 5330.849244279833}\n",
      "Iteration 20, Losses: {'ner': 5105.013301772962}\n",
      "Iteration 21, Losses: {'ner': 6077.5666464611995}\n",
      "Iteration 22, Losses: {'ner': 5618.436190295918}\n",
      "Iteration 23, Losses: {'ner': 4976.72116722649}\n",
      "Iteration 24, Losses: {'ner': 5195.682374777254}\n",
      "Iteration 25, Losses: {'ner': 4917.343004246664}\n",
      "Iteration 26, Losses: {'ner': 5084.555842489722}\n",
      "Iteration 27, Losses: {'ner': 4880.187193452441}\n",
      "Iteration 28, Losses: {'ner': 4865.721253295393}\n",
      "Iteration 29, Losses: {'ner': 4792.241013327795}\n",
      "Iteration 30, Losses: {'ner': 4810.359666515406}\n",
      "Iteration 31, Losses: {'ner': 4888.9427155118465}\n",
      "Iteration 32, Losses: {'ner': 4886.013001423022}\n",
      "Iteration 33, Losses: {'ner': 4783.489983031821}\n",
      "Iteration 34, Losses: {'ner': 4809.262524813307}\n",
      "Iteration 35, Losses: {'ner': 4623.28644184509}\n",
      "Iteration 36, Losses: {'ner': 4715.9783866473745}\n",
      "Iteration 37, Losses: {'ner': 4662.878134535653}\n",
      "Iteration 38, Losses: {'ner': 4770.838887770172}\n",
      "Iteration 39, Losses: {'ner': 4581.1532421311}\n",
      "Iteration 40, Losses: {'ner': 4617.934387466085}\n",
      "Iteration 41, Losses: {'ner': 4513.655950310953}\n",
      "Iteration 42, Losses: {'ner': 4651.910969067624}\n",
      "Iteration 43, Losses: {'ner': 4520.225975753373}\n",
      "Iteration 44, Losses: {'ner': 4645.011091756955}\n",
      "Iteration 45, Losses: {'ner': 4495.432877574449}\n",
      "Iteration 46, Losses: {'ner': 4587.390048702648}\n",
      "Iteration 47, Losses: {'ner': 4456.6254957435995}\n",
      "Iteration 48, Losses: {'ner': 4520.5743051922145}\n",
      "Iteration 49, Losses: {'ner': 4584.651555015741}\n",
      "Iteration 50, Losses: {'ner': 4529.515379691225}\n",
      "Iteration 51, Losses: {'ner': 4436.561682007948}\n",
      "Iteration 52, Losses: {'ner': 4481.782184234877}\n",
      "Iteration 53, Losses: {'ner': 4410.729594718174}\n",
      "Iteration 54, Losses: {'ner': 4639.222959817399}\n",
      "Iteration 55, Losses: {'ner': 4537.903974861791}\n",
      "Iteration 56, Losses: {'ner': 4408.348741786655}\n",
      "Iteration 57, Losses: {'ner': 4364.82100117428}\n",
      "Iteration 58, Losses: {'ner': 4466.991183339731}\n",
      "Iteration 59, Losses: {'ner': 4364.884635932281}\n",
      "Iteration 60, Losses: {'ner': 4380.54199085123}\n",
      "Iteration 61, Losses: {'ner': 4293.026084955567}\n",
      "Iteration 62, Losses: {'ner': 4490.689623595963}\n",
      "Iteration 63, Losses: {'ner': 4419.4267577884775}\n",
      "Iteration 64, Losses: {'ner': 4271.3069580310585}\n",
      "Iteration 65, Losses: {'ner': 4308.588943197516}\n",
      "Iteration 66, Losses: {'ner': 4321.555566702426}\n",
      "Iteration 67, Losses: {'ner': 4397.860490175504}\n",
      "Iteration 68, Losses: {'ner': 4307.424471099728}\n",
      "Iteration 69, Losses: {'ner': 4184.4646242311155}\n",
      "Iteration 70, Losses: {'ner': 4214.829288520352}\n",
      "Iteration 71, Losses: {'ner': 4193.041245354414}\n",
      "Iteration 72, Losses: {'ner': 4311.775375536018}\n",
      "Iteration 73, Losses: {'ner': 4160.621892807847}\n",
      "Iteration 74, Losses: {'ner': 4119.955660451096}\n",
      "Iteration 75, Losses: {'ner': 4284.041096946938}\n",
      "Iteration 76, Losses: {'ner': 4203.751631860025}\n",
      "Iteration 77, Losses: {'ner': 4185.008268372982}\n",
      "Iteration 78, Losses: {'ner': 4162.601561541446}\n",
      "Iteration 79, Losses: {'ner': 4085.3032774788526}\n",
      "Iteration 80, Losses: {'ner': 4076.0460568926255}\n",
      "Iteration 81, Losses: {'ner': 4143.762166742928}\n",
      "Iteration 82, Losses: {'ner': 4143.370562032065}\n",
      "Iteration 83, Losses: {'ner': 4184.247009170016}\n",
      "Iteration 84, Losses: {'ner': 4133.886511317946}\n",
      "Iteration 85, Losses: {'ner': 4078.5747676479828}\n",
      "Iteration 86, Losses: {'ner': 4032.941874094388}\n",
      "Iteration 87, Losses: {'ner': 4049.837111290874}\n",
      "Iteration 88, Losses: {'ner': 4154.895423012958}\n",
      "Iteration 89, Losses: {'ner': 3991.274916089822}\n",
      "Iteration 90, Losses: {'ner': 4088.056295821239}\n",
      "Iteration 91, Losses: {'ner': 4106.326121885015}\n",
      "Iteration 92, Losses: {'ner': 4013.2365775479425}\n",
      "Iteration 93, Losses: {'ner': 4085.232442571366}\n",
      "Iteration 94, Losses: {'ner': 4005.632978338228}\n",
      "Iteration 95, Losses: {'ner': 3927.26032432206}\n",
      "Iteration 96, Losses: {'ner': 3995.3908662794393}\n",
      "Iteration 97, Losses: {'ner': 4011.2409327772884}\n",
      "Iteration 98, Losses: {'ner': 3987.64861616978}\n",
      "Iteration 99, Losses: {'ner': 3968.508426095575}\n",
      "Iteration 100, Losses: {'ner': 4183.933858021374}\n",
      "Iteration 101, Losses: {'ner': 3959.322507043111}\n",
      "Iteration 102, Losses: {'ner': 3958.512143104996}\n",
      "Iteration 103, Losses: {'ner': 3810.954483246673}\n",
      "Iteration 104, Losses: {'ner': 3906.987903043873}\n",
      "Iteration 105, Losses: {'ner': 3942.5653751063774}\n",
      "Iteration 106, Losses: {'ner': 3966.246608704817}\n",
      "Iteration 107, Losses: {'ner': 3892.4361108631724}\n",
      "Iteration 108, Losses: {'ner': 3924.8474733480184}\n",
      "Iteration 109, Losses: {'ner': 3978.231910076619}\n",
      "Iteration 110, Losses: {'ner': 3898.624594701987}\n",
      "Iteration 111, Losses: {'ner': 3941.43251049868}\n",
      "Iteration 112, Losses: {'ner': 3874.335288568596}\n",
      "Iteration 113, Losses: {'ner': 3841.5604659446117}\n",
      "Iteration 114, Losses: {'ner': 3912.6803546660303}\n",
      "Iteration 115, Losses: {'ner': 3813.812557386051}\n",
      "Iteration 116, Losses: {'ner': 3865.423098742818}\n",
      "Iteration 117, Losses: {'ner': 3826.2604357744776}\n",
      "Iteration 118, Losses: {'ner': 3820.067018890082}\n",
      "Iteration 119, Losses: {'ner': 3873.7911848802632}\n",
      "Iteration 120, Losses: {'ner': 3862.4753150581023}\n",
      "Iteration 121, Losses: {'ner': 3720.969336616759}\n",
      "Iteration 122, Losses: {'ner': 3732.4772046032185}\n",
      "Iteration 123, Losses: {'ner': 3707.387762367653}\n",
      "Iteration 124, Losses: {'ner': 3743.417640195356}\n",
      "Iteration 125, Losses: {'ner': 3767.4462838357986}\n",
      "Iteration 126, Losses: {'ner': 3733.3829977775617}\n",
      "Iteration 127, Losses: {'ner': 3815.13001553879}\n",
      "Iteration 128, Losses: {'ner': 3939.5322679177402}\n",
      "Iteration 129, Losses: {'ner': 3733.3940952429894}\n",
      "Iteration 130, Losses: {'ner': 3829.444831666056}\n",
      "Iteration 131, Losses: {'ner': 3847.6211015603876}\n",
      "Iteration 132, Losses: {'ner': 3632.4319099684053}\n",
      "Iteration 133, Losses: {'ner': 3762.781395806312}\n",
      "Iteration 134, Losses: {'ner': 3731.4906278887242}\n",
      "Iteration 135, Losses: {'ner': 3723.011227351018}\n",
      "Iteration 136, Losses: {'ner': 3785.770870549981}\n",
      "Iteration 137, Losses: {'ner': 3739.36538376078}\n",
      "Iteration 138, Losses: {'ner': 3686.1517122567775}\n",
      "Iteration 139, Losses: {'ner': 3694.7016077860762}\n",
      "Iteration 140, Losses: {'ner': 3755.1974420947267}\n",
      "Iteration 141, Losses: {'ner': 3741.7897797906676}\n",
      "Iteration 142, Losses: {'ner': 3614.356774338168}\n",
      "Iteration 143, Losses: {'ner': 3559.835935264992}\n",
      "Iteration 144, Losses: {'ner': 3638.6896571180714}\n",
      "Iteration 145, Losses: {'ner': 3617.440271819237}\n",
      "Iteration 146, Losses: {'ner': 3670.337743922496}\n",
      "Iteration 147, Losses: {'ner': 3627.698933840412}\n",
      "Iteration 148, Losses: {'ner': 3663.429250687254}\n",
      "Iteration 149, Losses: {'ner': 3647.943960426957}\n",
      "Iteration 150, Losses: {'ner': 3666.809045743813}\n",
      "Iteration 151, Losses: {'ner': 3652.725797205661}\n",
      "Iteration 152, Losses: {'ner': 3660.4640826134114}\n",
      "Iteration 153, Losses: {'ner': 3680.1976886789726}\n",
      "Iteration 154, Losses: {'ner': 3727.948224233386}\n",
      "Iteration 155, Losses: {'ner': 3698.846406902182}\n",
      "Iteration 156, Losses: {'ner': 3736.1464845854525}\n",
      "Iteration 157, Losses: {'ner': 3725.1607949213044}\n",
      "Iteration 158, Losses: {'ner': 3600.657004527996}\n",
      "Iteration 159, Losses: {'ner': 3589.259327188556}\n",
      "Iteration 160, Losses: {'ner': 3535.1781703505276}\n",
      "Iteration 161, Losses: {'ner': 3777.291491912637}\n",
      "Iteration 162, Losses: {'ner': 3642.9903145205353}\n",
      "Iteration 163, Losses: {'ner': 3665.029237481345}\n",
      "Iteration 164, Losses: {'ner': 3626.2080598558814}\n",
      "Iteration 165, Losses: {'ner': 3455.901988730633}\n",
      "Iteration 166, Losses: {'ner': 3569.4993328421137}\n",
      "Iteration 167, Losses: {'ner': 3556.569606098566}\n",
      "Iteration 168, Losses: {'ner': 3627.427242594733}\n",
      "Iteration 169, Losses: {'ner': 3658.5682245331045}\n",
      "Iteration 170, Losses: {'ner': 3495.507012271072}\n",
      "Iteration 171, Losses: {'ner': 3524.71159921825}\n",
      "Iteration 172, Losses: {'ner': 3479.4190539102797}\n",
      "Iteration 173, Losses: {'ner': 3541.9415950516127}\n",
      "Iteration 174, Losses: {'ner': 3452.0852753619374}\n",
      "Iteration 175, Losses: {'ner': 3477.450400792684}\n",
      "Iteration 176, Losses: {'ner': 3484.6766689378783}\n",
      "Iteration 177, Losses: {'ner': 3501.9316579654733}\n",
      "Iteration 178, Losses: {'ner': 3383.2318878817678}\n",
      "Iteration 179, Losses: {'ner': 3501.345628885335}\n",
      "Iteration 180, Losses: {'ner': 3451.906754665206}\n",
      "Iteration 181, Losses: {'ner': 3457.768235711858}\n",
      "Iteration 182, Losses: {'ner': 3488.944139942653}\n",
      "Iteration 183, Losses: {'ner': 3463.0682560038704}\n",
      "Iteration 184, Losses: {'ner': 3489.53449909106}\n",
      "Iteration 185, Losses: {'ner': 3407.1915835342315}\n",
      "Iteration 186, Losses: {'ner': 3506.847050305061}\n",
      "Iteration 187, Losses: {'ner': 3489.1790415622722}\n",
      "Iteration 188, Losses: {'ner': 3431.880339063512}\n",
      "Iteration 189, Losses: {'ner': 3520.8256298454808}\n",
      "Iteration 190, Losses: {'ner': 3409.644754698056}\n",
      "Iteration 191, Losses: {'ner': 3414.817981656214}\n",
      "Iteration 192, Losses: {'ner': 3382.834468999795}\n",
      "Iteration 193, Losses: {'ner': 3468.194617718197}\n",
      "Iteration 194, Losses: {'ner': 3416.0206824923757}\n",
      "Iteration 195, Losses: {'ner': 3409.4735371784263}\n",
      "Iteration 196, Losses: {'ner': 3581.7156336057965}\n",
      "Iteration 197, Losses: {'ner': 3481.953660583782}\n",
      "Iteration 198, Losses: {'ner': 3488.0827489461135}\n",
      "Iteration 199, Losses: {'ner': 3434.404978840341}\n",
      "Iteration 200, Losses: {'ner': 3421.5215143584223}\n",
      "Iteration 201, Losses: {'ner': 3331.1754537876786}\n",
      "Iteration 202, Losses: {'ner': 3436.9998104375763}\n",
      "Iteration 203, Losses: {'ner': 3373.9117683921254}\n",
      "Iteration 204, Losses: {'ner': 3345.0896553010402}\n",
      "Iteration 205, Losses: {'ner': 3369.1072715306427}\n",
      "Iteration 206, Losses: {'ner': 3408.41102549817}\n",
      "Iteration 207, Losses: {'ner': 3334.968453228293}\n",
      "Iteration 208, Losses: {'ner': 3387.7160603055527}\n",
      "Iteration 209, Losses: {'ner': 3356.5693341097126}\n",
      "Iteration 210, Losses: {'ner': 3440.0861245280767}\n",
      "Iteration 211, Losses: {'ner': 3258.5759692624497}\n",
      "Iteration 212, Losses: {'ner': 3346.587508440073}\n",
      "Iteration 213, Losses: {'ner': 3298.6704959633994}\n",
      "Iteration 214, Losses: {'ner': 3445.0998427891973}\n",
      "Iteration 215, Losses: {'ner': 3326.8387482304406}\n",
      "Iteration 216, Losses: {'ner': 3238.7802731092297}\n",
      "Iteration 217, Losses: {'ner': 3328.4245289192004}\n",
      "Iteration 218, Losses: {'ner': 3233.3233824221443}\n",
      "Iteration 219, Losses: {'ner': 3282.866731941406}\n",
      "Iteration 220, Losses: {'ner': 3292.68142863521}\n",
      "Iteration 221, Losses: {'ner': 3311.8341269386547}\n",
      "Iteration 222, Losses: {'ner': 3476.969840313189}\n",
      "Iteration 223, Losses: {'ner': 3302.414844437473}\n",
      "Iteration 224, Losses: {'ner': 3282.790382985228}\n",
      "Iteration 225, Losses: {'ner': 3339.3180889068476}\n",
      "Iteration 226, Losses: {'ner': 3308.1602115322808}\n",
      "Iteration 227, Losses: {'ner': 3309.694991340258}\n",
      "Iteration 228, Losses: {'ner': 3295.1022731055223}\n",
      "Iteration 229, Losses: {'ner': 3285.982220276558}\n",
      "Iteration 230, Losses: {'ner': 3331.707206244125}\n",
      "Iteration 231, Losses: {'ner': 3322.1817702390704}\n",
      "Iteration 232, Losses: {'ner': 3274.8825336078826}\n",
      "Iteration 233, Losses: {'ner': 3328.763039754888}\n",
      "Iteration 234, Losses: {'ner': 3319.8308192230156}\n",
      "Iteration 235, Losses: {'ner': 3296.398368198214}\n",
      "Iteration 236, Losses: {'ner': 3267.433535455629}\n",
      "Iteration 237, Losses: {'ner': 3272.49904362663}\n",
      "Iteration 238, Losses: {'ner': 3302.7591131519503}\n",
      "Iteration 239, Losses: {'ner': 3310.0407781361705}\n",
      "Iteration 240, Losses: {'ner': 3348.4034073938865}\n",
      "Iteration 241, Losses: {'ner': 3275.8450618720904}\n",
      "Iteration 242, Losses: {'ner': 3257.9261578856235}\n",
      "Iteration 243, Losses: {'ner': 3199.256255356449}\n",
      "Iteration 244, Losses: {'ner': 3300.1854564522982}\n",
      "Iteration 245, Losses: {'ner': 3194.500107298649}\n",
      "Iteration 246, Losses: {'ner': 3257.738717774351}\n",
      "Iteration 247, Losses: {'ner': 3167.990380466424}\n",
      "Iteration 248, Losses: {'ner': 3224.5564253503176}\n",
      "Iteration 249, Losses: {'ner': 3184.072330674241}\n",
      "Iteration 250, Losses: {'ner': 3295.153303800182}\n",
      "Iteration 251, Losses: {'ner': 3335.0271079053396}\n",
      "Iteration 252, Losses: {'ner': 3242.6095169845053}\n",
      "Iteration 253, Losses: {'ner': 3202.906855957908}\n",
      "Iteration 254, Losses: {'ner': 3130.3239235453857}\n",
      "Iteration 255, Losses: {'ner': 3289.0052157681303}\n",
      "Iteration 256, Losses: {'ner': 3204.96624429241}\n",
      "Iteration 257, Losses: {'ner': 3269.755025875672}\n",
      "Iteration 258, Losses: {'ner': 3182.9903888765134}\n",
      "Iteration 259, Losses: {'ner': 3169.953650509616}\n",
      "Iteration 260, Losses: {'ner': 3225.786533006243}\n",
      "Iteration 261, Losses: {'ner': 3203.8688262182436}\n",
      "Iteration 262, Losses: {'ner': 3277.651338322935}\n",
      "Iteration 263, Losses: {'ner': 3175.4869722062517}\n",
      "Iteration 264, Losses: {'ner': 3287.267805940204}\n",
      "Iteration 265, Losses: {'ner': 3181.8461663919948}\n",
      "Iteration 266, Losses: {'ner': 3199.160789903196}\n",
      "Iteration 267, Losses: {'ner': 3200.586729794296}\n",
      "Iteration 268, Losses: {'ner': 3162.730978681693}\n",
      "Iteration 269, Losses: {'ner': 3167.610440523803}\n",
      "Iteration 270, Losses: {'ner': 3155.265186053701}\n",
      "Iteration 271, Losses: {'ner': 3163.1579104962384}\n",
      "Iteration 272, Losses: {'ner': 3094.7438489068722}\n",
      "Iteration 273, Losses: {'ner': 3117.797086283603}\n",
      "Iteration 274, Losses: {'ner': 3095.3783077013295}\n",
      "Iteration 275, Losses: {'ner': 3218.402145620841}\n",
      "Iteration 276, Losses: {'ner': 3120.6240987936226}\n",
      "Iteration 277, Losses: {'ner': 3089.6225819147603}\n",
      "Iteration 278, Losses: {'ner': 3172.3124579460155}\n",
      "Iteration 279, Losses: {'ner': 3126.224934172866}\n",
      "Iteration 280, Losses: {'ner': 3039.4993808476493}\n",
      "Iteration 281, Losses: {'ner': 3088.326615637031}\n",
      "Iteration 282, Losses: {'ner': 3045.5122001003483}\n",
      "Iteration 283, Losses: {'ner': 3118.2832771571957}\n",
      "Iteration 284, Losses: {'ner': 3303.945087273948}\n",
      "Iteration 285, Losses: {'ner': 3089.9361000119725}\n",
      "Iteration 286, Losses: {'ner': 3100.739458152142}\n",
      "Iteration 287, Losses: {'ner': 3119.922428200608}\n",
      "Iteration 288, Losses: {'ner': 3120.439806686038}\n",
      "Iteration 289, Losses: {'ner': 3136.589523845128}\n",
      "Iteration 290, Losses: {'ner': 3017.499626656695}\n",
      "Iteration 291, Losses: {'ner': 3076.020460853461}\n",
      "Iteration 292, Losses: {'ner': 3129.3766820524384}\n",
      "Iteration 293, Losses: {'ner': 3093.4112053829563}\n",
      "Iteration 294, Losses: {'ner': 3051.4183160944467}\n",
      "Iteration 295, Losses: {'ner': 3061.8641796261168}\n",
      "Iteration 296, Losses: {'ner': 3090.137061588941}\n",
      "Iteration 297, Losses: {'ner': 3196.6245269678416}\n",
      "Iteration 298, Losses: {'ner': 3179.4804523805733}\n",
      "Iteration 299, Losses: {'ner': 3073.417541260065}\n",
      "Iteration 300, Losses: {'ner': 3137.868818725136}\n",
      "Iteration 301, Losses: {'ner': 2988.3148027641446}\n",
      "Iteration 302, Losses: {'ner': 3011.984782538744}\n",
      "Iteration 303, Losses: {'ner': 3004.9002256686895}\n",
      "Iteration 304, Losses: {'ner': 3063.295586661864}\n",
      "Iteration 305, Losses: {'ner': 3056.158464964578}\n",
      "Iteration 306, Losses: {'ner': 3014.1256565729077}\n",
      "Iteration 307, Losses: {'ner': 3048.3510025072533}\n",
      "Iteration 308, Losses: {'ner': 3072.648222463689}\n",
      "Iteration 309, Losses: {'ner': 3107.738653998379}\n",
      "Iteration 310, Losses: {'ner': 3027.4952746388876}\n",
      "Iteration 311, Losses: {'ner': 3045.2118893559928}\n",
      "Iteration 312, Losses: {'ner': 3038.0991343392884}\n",
      "Iteration 313, Losses: {'ner': 2973.274651105497}\n",
      "Iteration 314, Losses: {'ner': 3057.0126445488377}\n",
      "Iteration 315, Losses: {'ner': 3004.844359502852}\n",
      "Iteration 316, Losses: {'ner': 2992.231513826963}\n",
      "Iteration 317, Losses: {'ner': 2934.873087393163}\n",
      "Iteration 318, Losses: {'ner': 2982.6253713554083}\n",
      "Iteration 319, Losses: {'ner': 3058.2911665556976}\n",
      "Iteration 320, Losses: {'ner': 3087.7323142845175}\n",
      "Iteration 321, Losses: {'ner': 3156.710344753303}\n",
      "Iteration 322, Losses: {'ner': 3027.7856090975956}\n",
      "Iteration 323, Losses: {'ner': 2955.96205193392}\n",
      "Iteration 324, Losses: {'ner': 3008.8901207579174}\n",
      "Iteration 325, Losses: {'ner': 3021.016289003814}\n",
      "Iteration 326, Losses: {'ner': 3011.220436270079}\n",
      "Iteration 327, Losses: {'ner': 2968.0585828832677}\n",
      "Iteration 328, Losses: {'ner': 2995.765283459251}\n",
      "Iteration 329, Losses: {'ner': 2990.264611861689}\n",
      "Iteration 330, Losses: {'ner': 2993.5952348639926}\n",
      "Iteration 331, Losses: {'ner': 3002.600332448841}\n",
      "Iteration 332, Losses: {'ner': 2970.3406624432523}\n",
      "Iteration 333, Losses: {'ner': 2986.470837027432}\n",
      "Iteration 334, Losses: {'ner': 3018.5448826715874}\n",
      "Iteration 335, Losses: {'ner': 3047.150759323824}\n",
      "Iteration 336, Losses: {'ner': 3042.613882160951}\n",
      "Iteration 337, Losses: {'ner': 3025.3580066167647}\n",
      "Iteration 338, Losses: {'ner': 2937.8374272284163}\n",
      "Iteration 339, Losses: {'ner': 3018.7696875557153}\n",
      "Iteration 340, Losses: {'ner': 3006.617751597751}\n",
      "Iteration 341, Losses: {'ner': 2974.935602677416}\n",
      "Iteration 342, Losses: {'ner': 2969.45360837182}\n",
      "Iteration 343, Losses: {'ner': 2979.722785584677}\n",
      "Iteration 344, Losses: {'ner': 3001.4999130851625}\n",
      "Iteration 345, Losses: {'ner': 3016.7431407644426}\n",
      "Iteration 346, Losses: {'ner': 3009.6584044337746}\n",
      "Iteration 347, Losses: {'ner': 2968.796525572534}\n",
      "Iteration 348, Losses: {'ner': 2994.2610405171777}\n",
      "Iteration 349, Losses: {'ner': 3059.058543239456}\n",
      "Iteration 350, Losses: {'ner': 3017.8437809086145}\n",
      "Iteration 351, Losses: {'ner': 3020.2369613059145}\n",
      "Iteration 352, Losses: {'ner': 2900.402946125374}\n",
      "Iteration 353, Losses: {'ner': 2889.1332159405083}\n",
      "Iteration 354, Losses: {'ner': 2970.9835520856636}\n",
      "Iteration 355, Losses: {'ner': 2996.3282694504214}\n",
      "Iteration 356, Losses: {'ner': 2942.6585466232864}\n",
      "Iteration 357, Losses: {'ner': 2927.4760092306365}\n",
      "Iteration 358, Losses: {'ner': 2957.2064994809816}\n",
      "Iteration 359, Losses: {'ner': 2929.6078521423156}\n",
      "Iteration 360, Losses: {'ner': 2949.8425971318284}\n",
      "Iteration 361, Losses: {'ner': 3006.585195632258}\n",
      "Iteration 362, Losses: {'ner': 2976.385368469564}\n",
      "Iteration 363, Losses: {'ner': 2917.9958545859363}\n",
      "Iteration 364, Losses: {'ner': 2999.243823998789}\n",
      "Iteration 365, Losses: {'ner': 2912.4780310542496}\n",
      "Iteration 366, Losses: {'ner': 2959.6332060348936}\n",
      "Iteration 367, Losses: {'ner': 2993.396640952339}\n",
      "Iteration 368, Losses: {'ner': 2986.5933836913823}\n",
      "Iteration 369, Losses: {'ner': 2913.4321905075176}\n",
      "Iteration 370, Losses: {'ner': 2922.545700759515}\n",
      "Iteration 371, Losses: {'ner': 2880.359421024396}\n",
      "Iteration 372, Losses: {'ner': 2909.7763207398457}\n",
      "Iteration 373, Losses: {'ner': 2898.590507671497}\n",
      "Iteration 374, Losses: {'ner': 2927.9057184516428}\n",
      "Iteration 375, Losses: {'ner': 2859.9520403815313}\n",
      "Iteration 376, Losses: {'ner': 2877.3753958727275}\n",
      "Iteration 377, Losses: {'ner': 2902.6488825415718}\n",
      "Iteration 378, Losses: {'ner': 3060.0487225363217}\n",
      "Iteration 379, Losses: {'ner': 2875.1215391960463}\n",
      "Iteration 380, Losses: {'ner': 2928.930777852681}\n",
      "Iteration 381, Losses: {'ner': 2860.9362951264175}\n",
      "Iteration 382, Losses: {'ner': 2863.742524032328}\n",
      "Iteration 383, Losses: {'ner': 2947.6860413904396}\n",
      "Iteration 384, Losses: {'ner': 2915.843008070965}\n",
      "Iteration 385, Losses: {'ner': 2885.703119854984}\n",
      "Iteration 386, Losses: {'ner': 2971.7306145634966}\n",
      "Iteration 387, Losses: {'ner': 2907.8470042168387}\n",
      "Iteration 388, Losses: {'ner': 2836.162646650156}\n",
      "Iteration 389, Losses: {'ner': 2920.404357259095}\n",
      "Iteration 390, Losses: {'ner': 2878.7450410521715}\n",
      "Iteration 391, Losses: {'ner': 2884.9161561550163}\n",
      "Iteration 392, Losses: {'ner': 2852.2818059898627}\n",
      "Iteration 393, Losses: {'ner': 2929.054164464761}\n",
      "Iteration 394, Losses: {'ner': 2981.7546260372446}\n",
      "Iteration 395, Losses: {'ner': 2952.2017229045678}\n",
      "Iteration 396, Losses: {'ner': 2853.4386250830107}\n",
      "Iteration 397, Losses: {'ner': 2880.9217438931023}\n",
      "Iteration 398, Losses: {'ner': 2843.388641512132}\n",
      "Iteration 399, Losses: {'ner': 2880.382885234225}\n",
      "Iteration 400, Losses: {'ner': 3013.8791000465717}\n",
      "Iteration 401, Losses: {'ner': 2877.739943780803}\n",
      "Iteration 402, Losses: {'ner': 2860.4524952847446}\n",
      "Iteration 403, Losses: {'ner': 2925.7868158113183}\n",
      "Iteration 404, Losses: {'ner': 2916.2089237761734}\n",
      "Iteration 405, Losses: {'ner': 2849.2616237349876}\n",
      "Iteration 406, Losses: {'ner': 2833.295870157177}\n",
      "Iteration 407, Losses: {'ner': 2867.692807919001}\n",
      "Iteration 408, Losses: {'ner': 2856.3221117734956}\n",
      "Iteration 409, Losses: {'ner': 2909.1056408503123}\n",
      "Iteration 410, Losses: {'ner': 2778.8632780105427}\n",
      "Iteration 411, Losses: {'ner': 2888.2215374667835}\n",
      "Iteration 412, Losses: {'ner': 2898.6550983899265}\n",
      "Iteration 413, Losses: {'ner': 2878.293722131918}\n",
      "Iteration 414, Losses: {'ner': 2828.795328113948}\n",
      "Iteration 415, Losses: {'ner': 2870.4511103341106}\n",
      "Iteration 416, Losses: {'ner': 2861.32120734354}\n",
      "Iteration 417, Losses: {'ner': 2843.1643464178874}\n",
      "Iteration 418, Losses: {'ner': 2879.560217046186}\n",
      "Iteration 419, Losses: {'ner': 2901.686981999142}\n",
      "Iteration 420, Losses: {'ner': 2808.6356342208173}\n",
      "Iteration 421, Losses: {'ner': 2819.797424798799}\n",
      "Iteration 422, Losses: {'ner': 2745.9084352261625}\n",
      "Iteration 423, Losses: {'ner': 2784.190705783195}\n",
      "Iteration 424, Losses: {'ner': 2842.351421299025}\n",
      "Iteration 425, Losses: {'ner': 2789.743623805375}\n",
      "Iteration 426, Losses: {'ner': 2846.0029285733663}\n",
      "Iteration 427, Losses: {'ner': 2805.335339783001}\n",
      "Iteration 428, Losses: {'ner': 2855.707296095345}\n",
      "Iteration 429, Losses: {'ner': 2833.835640577369}\n",
      "Iteration 430, Losses: {'ner': 2749.0160265728623}\n",
      "Iteration 431, Losses: {'ner': 2854.0306221444825}\n",
      "Iteration 432, Losses: {'ner': 2755.2268462604247}\n",
      "Iteration 433, Losses: {'ner': 2762.0333947369877}\n",
      "Iteration 434, Losses: {'ner': 2894.9777091815104}\n",
      "Iteration 435, Losses: {'ner': 2850.4506720326863}\n",
      "Iteration 436, Losses: {'ner': 2806.4200677624194}\n",
      "Iteration 437, Losses: {'ner': 2885.691727492707}\n",
      "Iteration 438, Losses: {'ner': 2929.584286571171}\n",
      "Iteration 439, Losses: {'ner': 2913.78761549288}\n",
      "Iteration 440, Losses: {'ner': 2828.2177771329025}\n",
      "Iteration 441, Losses: {'ner': 2758.079698946418}\n",
      "Iteration 442, Losses: {'ner': 2825.456539972423}\n",
      "Iteration 443, Losses: {'ner': 2874.1940502568477}\n",
      "Iteration 444, Losses: {'ner': 2880.3416859692506}\n",
      "Iteration 445, Losses: {'ner': 2845.734791656765}\n",
      "Iteration 446, Losses: {'ner': 2946.2304358323017}\n",
      "Iteration 447, Losses: {'ner': 2836.4747433006564}\n",
      "Iteration 448, Losses: {'ner': 2799.134326425843}\n",
      "Iteration 449, Losses: {'ner': 2817.162096484593}\n",
      "Iteration 450, Losses: {'ner': 2820.575115481461}\n",
      "Iteration 451, Losses: {'ner': 2867.230758425246}\n",
      "Iteration 452, Losses: {'ner': 2854.560980950846}\n",
      "Iteration 453, Losses: {'ner': 2793.234761254763}\n",
      "Iteration 454, Losses: {'ner': 2788.826470099334}\n",
      "Iteration 455, Losses: {'ner': 2758.4976378422134}\n",
      "Iteration 456, Losses: {'ner': 2852.0997526872206}\n",
      "Iteration 457, Losses: {'ner': 2755.287297792923}\n",
      "Iteration 458, Losses: {'ner': 2699.6422059795095}\n",
      "Iteration 459, Losses: {'ner': 2806.355659203454}\n",
      "Iteration 460, Losses: {'ner': 2748.724262417496}\n",
      "Iteration 461, Losses: {'ner': 2771.710543263712}\n",
      "Iteration 462, Losses: {'ner': 2716.017717160827}\n",
      "Iteration 463, Losses: {'ner': 2767.588900450296}\n",
      "Iteration 464, Losses: {'ner': 2823.1653281678623}\n",
      "Iteration 465, Losses: {'ner': 2823.8081222396245}\n",
      "Iteration 466, Losses: {'ner': 2782.5947753700802}\n",
      "Iteration 467, Losses: {'ner': 2846.568215377227}\n",
      "Iteration 468, Losses: {'ner': 2820.6442646814307}\n",
      "Iteration 469, Losses: {'ner': 2845.9472219370073}\n",
      "Iteration 470, Losses: {'ner': 2748.8863213117493}\n",
      "Iteration 471, Losses: {'ner': 2801.694819195316}\n",
      "Iteration 472, Losses: {'ner': 2730.7933599181215}\n",
      "Iteration 473, Losses: {'ner': 2771.917863641646}\n",
      "Iteration 474, Losses: {'ner': 2716.014437387715}\n",
      "Iteration 475, Losses: {'ner': 2756.2872314409287}\n",
      "Iteration 476, Losses: {'ner': 2714.0910260795663}\n",
      "Iteration 477, Losses: {'ner': 2675.0287460726727}\n",
      "Iteration 478, Losses: {'ner': 2683.264878012579}\n",
      "Iteration 479, Losses: {'ner': 2716.3190268012095}\n",
      "Iteration 480, Losses: {'ner': 2767.9414466808857}\n",
      "Iteration 481, Losses: {'ner': 2744.920149031355}\n",
      "Iteration 482, Losses: {'ner': 2764.0537623747546}\n",
      "Iteration 483, Losses: {'ner': 2682.4550574682185}\n",
      "Iteration 484, Losses: {'ner': 2835.7541197598603}\n",
      "Iteration 485, Losses: {'ner': 2759.659721650482}\n",
      "Iteration 486, Losses: {'ner': 2764.208649309645}\n",
      "Iteration 487, Losses: {'ner': 2740.0512315879178}\n",
      "Iteration 488, Losses: {'ner': 2728.696308546695}\n",
      "Iteration 489, Losses: {'ner': 2721.614271359213}\n",
      "Iteration 490, Losses: {'ner': 2785.7083085672334}\n",
      "Iteration 491, Losses: {'ner': 2751.912601337398}\n",
      "Iteration 492, Losses: {'ner': 2764.4909680483806}\n",
      "Iteration 493, Losses: {'ner': 2639.597231485524}\n",
      "Iteration 494, Losses: {'ner': 2776.6716894323613}\n",
      "Iteration 495, Losses: {'ner': 2761.1622925306256}\n",
      "Iteration 496, Losses: {'ner': 2685.0307275075047}\n",
      "Iteration 497, Losses: {'ner': 2740.273207826973}\n",
      "Iteration 498, Losses: {'ner': 2703.181596926902}\n",
      "Iteration 499, Losses: {'ner': 2735.6701301960748}\n",
      "Iteration 500, Losses: {'ner': 2688.643979115844}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def validate_data(text, entities):\n",
    "    for start, end, label in entities:\n",
    "        if not isinstance(start, int) or not isinstance(end, int):\n",
    "            raise ValueError(f\"Entity spans must be integers. Got: start={start}, end={end}\")\n",
    "        if start < 0 or end > len(text):\n",
    "            raise ValueError(f\"Entity spans are out of range for the given text. Got: start={start}, end={end}, text length={len(text)}\")\n",
    "        if start >= end:\n",
    "            raise ValueError(f\"Entity start must be less than end. Got: start={start}, end={end}\")\n",
    "    return True\n",
    "\n",
    "def convert_data(data):\n",
    "    training_data = []\n",
    "    annotations = data.get(\"annotations\", [])\n",
    "    for item in annotations:\n",
    "        if item is None or not isinstance(item, list) or len(item) < 2:\n",
    "            continue\n",
    "        text = item[0]\n",
    "        annotations_dict = item[1]\n",
    "        entities = [(start, end, label) for start, end, label in annotations_dict.get(\"entities\", [])]\n",
    "        # Validate data\n",
    "        validate_data(text, entities)\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "# Directory containing JSON files\n",
    "data_directory = r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\ner-label\\OUTPUT\"\n",
    "\n",
    "# Collect all training data\n",
    "all_training_data = []\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        data = load_json_data(file_path)\n",
    "        training_data = convert_data(data)\n",
    "        all_training_data.extend(training_data)\n",
    "\n",
    "# Create a blank spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the NER component and add it to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add new labels to the NER component\n",
    "for _, annotations in all_training_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipeline components (if any)\n",
    "pipe_exceptions = [\"ner\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Training the NER model\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for iteration in range(500):\n",
    "        random.shuffle(all_training_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(all_training_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
    "            nlp.update(examples, drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"trained_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" to carryout day to day microbiological analysis ...\" with entities \"[(25, 49, 'JOB TITLE'), (511, 543, 'QUALIFICATION'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" job profile  day shift & 5 days a week respondin...\" with entities \"[(238, 259, 'EXPERIENCE'), (264, 277, 'EXPERIENCE'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" medical coding fresher required. day shift job. ...\" with entities \"[(2, 16, 'JOB TITLE'), (108, 135, 'SALARY'), (253,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" kindly apply after reading this post thoroughly....\" with entities \"[(97, 106, 'LOCATION'), (295, 303, 'EXPERIENCE'), ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.our company has got good cnc machines.. 2. we ...\" with entities \"[(68, 94, 'JOB TITLE'), (100, 130, 'SKILL '), (288...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.qualifications  essential  bachelor degree des...\" with entities \"[(122, 128, 'LOCATION'), (131, 139, 'LOCATION'), (...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" position title assistant product manager / produ...\" with entities \"[(2065, 2079, 'SALARY'), (2082, 2094, 'SALARY'), (...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" ra api salary  not disclosed by recruiter indust...\" with entities \"[(54, 60, 'INDUSTRY'), (63, 70, 'INDUSTRY'), (73, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" analysis and solution definition quickly underst...\" with entities \"[(995, 1008, 'SKILL'), (1192, 1207, 'SKILL'), (137...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" medical coding is the process of conversion of t...\" with entities \"[(214, 242, 'SALARY'), (302, 309, 'INDUSTRY'), (31...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" 1.will effectively collaborate with the recruite...\" with entities \"[(724, 745, 'SKILL'), (778, 790, 'SKILL'), (802, 8...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" ideal candidate is someone who is very smart, a ...\" with entities \"[(209, 216, 'EXPERIENCE'), (508, 512, 'SKILL'), (5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" our client is the largest beauty services market...\" with entities \"[(946, 948, 'SKILL'), (950, 957, 'SKILL'), (959, 9...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" greetings from pace recruitment    urgent openin...\" with entities \"[(56, 87, 'JOB_TITLE'), (115, 124, 'LOCATION'), (2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" location  waverock sez, hyderabad department  te...\" with entities \"[(26, 35, 'LOCATION'), (120, 182, 'SKILL'), (214, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" health care services, admin, team building, nurs...\" with entities \"[(2, 29, 'JOB_TITLE'), (46, 53, 'JOB_TITLE'), (55,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aachi\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\" name of the client reputed engineering college ,...\" with entities \"[(105, 118, 'EXPERIENCE'), (177, 187, 'LOCATION'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Evaluation Results:\n",
      "Precision: 0.80\n",
      "Recall: 0.79\n",
      "F1-Score: 0.79\n",
      "\n",
      "Scores by entity type:\n",
      "LOCATION:\n",
      "  Precision: 0.89\n",
      "  Recall: 0.95\n",
      "  F1-Score: 0.92\n",
      "INDUSTRY:\n",
      "  Precision: 0.69\n",
      "  Recall: 0.70\n",
      "  F1-Score: 0.69\n",
      "SKILL :\n",
      "  Precision: 0.85\n",
      "  Recall: 0.88\n",
      "  F1-Score: 0.86\n",
      "EXPERIENCE:\n",
      "  Precision: 0.95\n",
      "  Recall: 0.97\n",
      "  F1-Score: 0.96\n",
      "SKILL:\n",
      "  Precision: 0.87\n",
      "  Recall: 0.85\n",
      "  F1-Score: 0.86\n",
      "JOB_TITLE:\n",
      "  Precision: 0.65\n",
      "  Recall: 0.44\n",
      "  F1-Score: 0.52\n",
      "QUALIFICATION:\n",
      "  Precision: 0.68\n",
      "  Recall: 0.65\n",
      "  F1-Score: 0.66\n",
      "JOB TITLE:\n",
      "  Precision: 0.79\n",
      "  Recall: 0.80\n",
      "  F1-Score: 0.79\n",
      "SALARY:\n",
      "  Precision: 0.98\n",
      "  Recall: 0.99\n",
      "  F1-Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to load test data\n",
    "def load_data(data_directory):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            for item in data.get(\"annotations\", []):\n",
    "                if item is None:\n",
    "                    continue\n",
    "                text, annotations = item\n",
    "                entities = [(start, end, label) for start, end, label in annotations.get(\"entities\", [])]\n",
    "                all_data.append((text, {\"entities\": entities}))\n",
    "    return all_data\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(nlp, test_data):\n",
    "    examples = []\n",
    "    for text, annotations in test_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    \n",
    "    results = nlp.evaluate(examples)\n",
    "    return results\n",
    "\n",
    "# Load the trained model\n",
    "model_path = r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\trained_model\"\n",
    "nlp = spacy.load(model_path)\n",
    "\n",
    "# Load test data\n",
    "test_data_directory = r\"E:\\Sem 2\\Natural Language processing\\PRESENTATION\\Public job posting\\ner-label\\OUTPUT\"\n",
    "test_data = load_data(test_data_directory)\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(nlp, test_data)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"NER Evaluation Results:\")\n",
    "if results.get('ents_p') is not None:\n",
    "    print(f\"Precision: {results['ents_p']:.2f}\")\n",
    "    print(f\"Recall: {results['ents_r']:.2f}\")\n",
    "    print(f\"F1-Score: {results['ents_f']:.2f}\")\n",
    "else:\n",
    "    print(\"No entities were predicted. Please check your test data and model.\")\n",
    "\n",
    "# Print scores for individual entity types\n",
    "print(\"\\nScores by entity type:\")\n",
    "if results.get('ents_per_type'):\n",
    "    for entity_type, scores in results['ents_per_type'].items():\n",
    "        print(f\"{entity_type}:\")\n",
    "        print(f\"  Precision: {scores['p']:.2f}\")\n",
    "        print(f\"  Recall: {scores['r']:.2f}\")\n",
    "        print(f\"  F1-Score: {scores['f']:.2f}\")\n",
    "else:\n",
    "    print(\"No entity types were evaluated. Please check your test data and model.\")\n",
    "\n",
    "# # Print some example predictions\n",
    "# print(\"\\nExample Predictions:\")\n",
    "# for text, _ in test_data[:5]:  # Print predictions for first 5 examples\n",
    "#     doc = nlp(text)\n",
    "#     print(f\"\\nText: {text}\")\n",
    "#     print(\"Predicted Entities:\")\n",
    "#     for ent in doc.ents:\n",
    "#         print(f\"  {ent.text} - {ent.label_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
